%% bare_conf.tex
%% V1.4a
%% 2014/09/17
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8a or later) with an IEEE
%% conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE!
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_conf_compsoc.tex,
%%                    bare_jrnl_compsoc.tex, bare_jrnl_transmag.tex
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices and paper sizes can       ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[conference]{IEEEtran}
% Some Computer Society conferences also require the compsoc mode option,
% but others use the standard conference format.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  %declare the path(s) where your graphic files are
  \graphicspath{{./}}
  %and their extensions so you won't have to specify these with
  %every instance of \includegraphics
  \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/tex-archive/info/epslatex/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage[cmex10]{amsmath}
\usepackage{amssymb}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Do not use the stfloats baselinefloat ability as IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/dblfloatfix/




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/url/
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Literature Review \\
Efficient KNN Join Algorithms}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Ramanathan Sivagurunathan}
\IEEEauthorblockA{Sydney University\\
Sydney\\
Email: rsiv5112@syd.uni.edu}}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
%
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3},
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
Review of all available works in KNN Join and their current relevance
in the age of big data and distributed computing
\end{abstract}

% no keywords




% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
% no \IEEEPARstart

KNN (K Nearest Neighbour) Join is one of the simplest and elegant
classification or regression algorithm which works remarkably well in
practice. It is a non parametric lazy learning algorithm. It is
classified as one of the top 10 machine learning algorithm. KNN Join
has become an important primitive in datamining and
finds its application in multimedia data retrieval, Online
recommendation, DNA search,
Molecular Biology, bioinformatics  and
many other fields.

\medskip

KNN Join is a computation intensive algorithm does
not scale for large datasets. Since most of the current works in
improving KNN Join are single node oriented, we cannot use KNN Join for datasets which does not fit into single nodes.
With evergrowing datasets, the papers \cite{zhang_efficient_2012}
\cite{lu_efficient_2012} \cite{stupar_rankreduceprocessing_2010} make use of
Hadoop mapreduce framework \cite{_hadoop_mr} for KNN Join. But still there
are no robust implementation present in any distributed machine learning
libraries like apache mahout\cite{_apache_mahout} and spark mLlib\cite{_spark_mLlib}. In this literature review we
will evaluate few of the KNN Join algorithm that are available.

\section{KNN Join}

\emph{Definition:1} Given two datasets R and S, where $R \in \mathbb{R}^d\ and\ S\ \in\ \mathbb{R}^d$, KNN Join returns the K
Nearest Neighbours in S for every feature Vector in R.

\bigskip
     $KNN(R,S) = {(r, KNN(r,S), \forall r\ \in\ \mathbb{R}^d)}$
\bigskip

\emph{Definition:2} Given two datasets R and S where $R\ \in\
\mathbb{R}^d\ and\ S\ \in\ \mathbb{R}^d$, Approximate KNN Join returns the K
approximate neighbours in S for every feature Vector in R.

\medskip

If $p$ is the exact K\textsuperscript{th} neighbour of r and $p'$ be approximate
K\textsuperscript{th} neighbour of r, then for any $\epsilon$

\bigskip

$d(p,r) <= d(p',r) <= (1+\epsilon) d(p,r)$

\bigskip

The computational complexity of KNN Join is
$O(\|R\| \times \|S\|)$. Computation cost of KNN Join is very high, which
deters everyone to use KNN Join in any application where
the number of elements in dataset R and S is large. Until late 1990s, KNN Join was
never considered a feasible algorithm at all due to its computational
complexity. There are significant amount of research that has been done to reduce the
computational complexity. Most algorithms that are out today have focussed
on either Indexing Method or Paritioning Method in a standalone system. This is because most of those research predates the
current big data era. In this modern day of high velocity and volume of data,
only distributed approach will yield the results in resonable amount of time.

\bigskip

\begin{table}[!t]
%\renewcommand{\arraystretch}{1.3}
\caption{Notations}
\label{notations}
\centering
\begin{tabular}{|c|l|}
\hline
Notation & Meaning \\
\hline
R and S & Dataset\\

k & Number of neighbour points \\

d & Number of dimensions \\

n & Number of elements in Dataset \\

c & some positive constant \\

$R_i, S_i$ & Partitions in R and S Respectively \\

r,s & feature Vectors in Dataset R and S Respectively \\

d(r,s) & Distance between two points r and s \\

$O_i$ & Pivot points \\

\hline
\end{tabular}
\end{table}

\subsection{Methods}
The current available research for reducing the computational
complexity can be broadly classified into these methodology.
\begin{enumerate}
\item Index Based
\item Partition Based
\end{enumerate}

\medskip

In Index based KNN Join, an index is created for one or both R and S
dataset. Index is used a primary way of reducing complexity in \cite{jagadish_idistance:_2005}
\cite{yu_efficient_2007} \cite{bohm_epsilon_2001}.  With indexes it
becomes easier to look for neighbouring elements in Dataset S, based
on certain features like distance, boundary cells,  as we
need to compare only a subset of elements which satisty the
criteria. For example, In distance based index
\cite{jagadish_idistance:_2005} we only compare elements which are at
similar distance in S for an element in R. This can result in huge
reduction in the number of comparison and it can be as low as $log(n)$, where n is the number of element is
dataset S.

\medskip

Indexes does not scale for high dimension data. This is true for various
index algorithms like R tree, $R^* tree$, Quad Tree, Mux tree, B+
tree. Using index for reducing distance compuation with every element, works
remarkably well for 2D or 3D geospatial data. The major drawback with the
index based approach is that when the number of dimensions increase
creating an index is much costly and its performance is similar to
that of sequential scan. This is because at higher dimensions the distance
between closest and farthest neighbors will be nearly same
\cite{beyer_when_1999}. Another key problem with this approach, even
for lower dimensions, when
converting to distributed environment is that it is difficult to
create an index and use it.

\bigskip

In Partition based KNN Join, the dataset is partitioned into different
non overlapping partitions. The
partition can be distance based\cite{lu_efficient_2012} or hashing
based \cite{stupar_rankreduceprocessing_2010} \cite{yao_k_2010}. For each partition in R only few partitions in S which are
near to R is considered for presence of Nearest Neighbours. This
method quickly simplifies the number of elements which we need to
compare when we are looking for neighbours.

\medskip

There are few challenges with this approach as well. One
of the main challenge is to find a way to partition all nearby neighbours for a subset
$R_i$ in relatively few subsets of $S_i$. Hash based partition cannot
get partition correctly, which means hash based partition will not
provide accurate KNN Join results. In distance based partition it is difficult to partition
with relatively low standard deviation in size of the partition. If
not partitioned properly it can result in one big partition which
would result in same performance as that of bruteforce.

\bigskip

\subsection{Centralised vs Distributed}

Since most work in KNN Join has been done before the big data era,
the main limitations of most algorithms are that they are centralised
\cite{jagadish_idistance:_2005} \cite{xia_gorder:_2004}
\cite{bohm_k-nearest_2004} \cite{kuan_fast_1997} \cite{yu_high-dimensional_2010}.
Several recent studies \cite{stupar_rankreduceprocessing_2010},
\cite{lu_efficient_2012} have focused on distributed
processing, which enables those algorithms to be used in bigger
dataset which is not possible otherwise.

\medskip

Hadoop Mapreduce was dominant framework for past 10 years. But as the
systems evolve, mapreduce framework is on the way out
too. Now spark is quickly becoming the defacto framework for
distributed computing. There is significant increase in number users
in spark and it has proved its exceptional performance over
hadoop mapreduce. But there are no work that has been done to make
KNN Join better for spark framework. This is an area where significant
work is needed to convert algorithms in centralized settings and mapreduce framework  to spark
framework.

\subsection{Accuracy}
Algorithms that computes KNN Join can provide accurate or approximate results. There
are trade off between accuracy and the computation cost. There are few
works dedicated to get approximate results \cite{stupar_rankreduceprocessing_2010}
\cite{zhang_efficient_2012} and few for accurate
\cite{jagadish_idistance:_2005} \cite{xia_gorder:_2004}
\cite{lu_efficient_2012}

\medskip

One of the major problem with approximate algorithms is that at higher
dimensions the accuracy becomes very low. The $\epsilon$ paramter to
control the level of accuracy, if it is very low can result in very
poor performance than the accurate one. Only the applications that is
using the algorithm can decide if such trade off is acceptable.

\medskip

Two methods are predominantly
used in approximate algorithms.
\begin{enumerate}
\item z order curve
\item Locality sensitive Hashing
\end{enumerate}

\bigskip

\section{Distributed Frameworks}

There are couple of dominant framework for distributed processing.
\begin{enumerate}
\item Hadoop Mapreduce Framework \cite{_hadoop_mr}
\item Spark Framework \cite{_apache_spark}
\end{enumerate}

\subsection{Hadoop Mapreduce}
Mapreduce\cite{dean_mapreduce:_2008} is a programming paradigm for processing large datasets on a
very large cluster. The model is inspired by functional programming concepts like
map and reduce. Mapreduce is primarily for batch processing at a
petabyte scale. Figure ~\ref{fig:hadooparch} from \cite{_hadoop_mr_arch} gives an overview of the architecture of
mapreduce.

\begin{figure}[here]
\includegraphics[width=0.5\textwidth]{hadooparch.jpg}
\caption{Hadoop Mapreduce Framework}
\label{fig:hadooparch}
\end{figure}

This model basically has two phase
\begin{enumerate}
\item Map phase
\item Reduce phase
\end{enumerate}

The input data is partitioned based on fixed size by the mapreduce
framework and each partitioned
data is fed to the Map phase. In Map phase the partitioned input data
is converted into tuples (key value
pairs). This data is written to distributed filesystem.
There can an optional combiner in the map phase as well. This
is to reduce number of bytes written and hence the number of shuffle
bytes. This is important because the
primary focus on distributed processing nowadays is to preserve
network bandwidth.

\medskip

The mapreduce framework before feeding the output of map phase to
reduce does the following
\begin{enumerate}
\item shuffle bytes from map to reducuer
\item Sort all data with the same key
\end{enumerate}

Reduce phase processes each key along with all its values. It
processes the key and values and reduces to final
output and it is written to the distributed filesystem as well.
That inturn can be fed to another mapreduce job. It is called
chaining. It is normal to chain many mapreduce job in succession.

\bigskip

\subsection{Spark}
Spark is an opensource framework for cluster computing developed in
AMPLab at UCBerkeley. It is a generic framework for any large scale
data processing. One of the main idea in spark's design is to leverage
the amount of RAM available across nodes for computing. Leveraging this, spark's in memory primitive
makes it 10 to 100 times faster than hadoop mapreduce.

\medskip

Spark has a primitive in memory data abstraction called Resilient Distributed
Dataset (RDD). RDD has multiple partitions and each partition is
loaded in separate nodes. RDD can be created from any data sources including
HDFS, Cassandra, HBase and S3. RDD Spark provides a set of transformation on
RDD like map, filter, groupBy, sortBy etc. All the transformations
happen parallel on partitions, providing scalability. Since every transformation
is in memory the performance is much faster than Hadoop Mapreduce.

\subsection{Algorithm Design Requirements}

Though these framework are very generic, you cannot run any algorithm
readily in these frameworks. To design an efficient algorithms for these framworks,
Some of the factors to be considered are

\medskip

\begin{enumerate}
\item Algorithm has to divide data into partitions and handle each partition
  separately
\item There can be common data that can be shared among all the nodes
  but it has to be small
\item Amount of data replication should be minimised. This reduces
  disk IO and increases performance.
\item Amount of data shuffling has to be minimised. This helps in
  preserving network bandwidth
\item Increasing the size of cluster should be able to reduce run time
\end{enumerate}


\section{Algorithm Analysis}
In this section we will review various algorithm and analyse its
suitability for processing huge dataset. we will evaluate various
algorithms based on the following factors.
\begin{enumerate}
\item Size of R and S Datasets ($1 < n < 10^9$)
\item Number of Dimensions ($2 < d < c*10^4$)
\item Horizontal scalability ($1 < nodes < 1000$)
\item Shuffle Bytes
\item Time to complete
\item Accuracy of results
\end{enumerate}

\bigskip
\subsection{iDistance}
One of the early works on KNN Join is \cite{jagadish_idistance:_2005}, in which the authors
introduce a distance based indexing called as iDistance. By using
distance as a measure n dimensional data is converted into single
dimension. They have also used B+ tree as the underlying structure for indexing. By using B+ tree, the
algorithm can be potentially implemented over existing relational databases.

\bigskip
The iDistance algorithm is based on the two key ideas
\begin{enumerate}
\item If two points are similar then they will be similar to a
  reference point as well
\item Can convert any n dimensional point to single dimension by using
  the distance to the reference point
\end{enumerate}

\medskip

The distance function employed is euclidean. We can use other methods
like hamming, manhattan as well. The euclidean distance between two
points p1 = $(x_0, x_1, ... , x_d)$ and p2 = $(y_0, y_1, ..., y_d)$

\bigskip
dist(p1, p2) = $\sqrt{(x_0 - y_0)^2 + (x_1 - y_1)^2 + ... (x_d - y_d)^2}$
\bigskip

In iDistance algorithm, has following steps for creating the index
\begin{enumerate}
\item N pivots $O_0, O_1 ... O_n$ are chosen either randomly or by finding N clusters'
  center point / edge point.
\item The entire dataset is divided into N partition by finding the
  closest pivot to the point
\item For each point an index key y is obtained by
$y = i*c + dist(p, O_i)$, where c is a constant to make y from
different partition non overlapping
\item Create a B+ Tree to index the transformed points and also create
  an array to store the m pivots with smallest and largest distance
\end{enumerate}

\bigskip

Steps for querying Nearest Neighbours for point q are
\begin{enumerate}
\item Select a $\delta{x}$ as query distance
\item Find partitions that are overlapping with q + $\delta{x}$ as
  radius
\item For the selected partition search for K Nearest neighbours, if
  found stop else increase the $\delta{x}$ and continue step 2 and
  3.
\end{enumerate}

\bigskip
The advantage of this algorithm is that it can be used for both
approximate and accurate calculations by controlling the query
distance $\delta{x}$. Also for a small dataset which can fit to a node
we can calculate point queries very fast.

Some of the drawbacks for this algorithm are
\begin{enumerate}
\item This algorithm is designed for point queries (Size of R
  is very small). If two points P1 and P2 are very close in R, we donot reuse the query
distance calculation of P1 for P2
\item For a large dataset calculating the pivots using clustering algorithm are
  computationally intensive themselves
\item Cannot be used in Mapreduce or Spark readily as creating an
  index in a distributed data is challenging.
\end{enumerate}

\bigskip

\subsection{Analysis: iJoin}
In \cite{yu_efficient_2007}, the authors worked on improving iDistance queries
\cite{jagadish_idistance:_2005}. They have worked on an improvement for the drawback 1 stated
above with their algorithm called iJoin. In this method they have
partitioned both R and S dataset based on few fixed points called
pivots. Since they partition both R and S dataset based on the same
pivots, we can compute nearby partition for $R_i$ in $S$. By this we
can effectively prune other partitions that are far away. This helps
in reducing the computational complexity to a great extent.

\bigskip

The authors also propose two optimisation and they call it as iJoinAC
and iJoinDR. These two optimisations are primarily to reduce the
number of floating point calculations done if the number of dimensions
are very high, in the order of 1000s. In iJoinAC each datapoint in R
and S are represented by an approximate bounding cube which are
integers vectors rather than the floating point vectors. The iDistance
is calculated for the bounding cube and when traversal is done to find
nearest neighbours. Since they are integers the calculation is simpler
and faster. We only access the real floating point vectors once we have reached the
right bounding cube where we are expected to find the nearest
neighbours. The fig ~\ref{fig:iJoinAC} is taken from the original
paper itself.

\begin{figure}[here]
\includegraphics[width=0.5\textwidth]{iJoinAC.png}
\caption{iDistance structure with approximate cube}
\label{fig:iJoinAC}
\end{figure}

In iJoinDR, the dimensions are reduced by applying PCA for the
vectors and selecting the few dimensions where there are maximum
variability. The resulting low dimension vectors
are used for constructing the iDistance index. For example a well
known PCA algorithm called SVD has a computational complexity of
$O(N * d^2)$ where N is the number of datapoints and d number of
dimensions.

\bigskip

As its predecessor, the algorithm is not designed for distributed
datasets. The use of PCA in iJoinDR requires a significant increase in
computation if the dataset is huge. There is no study being done on
impact of approximate PCA results on the algorithm. This would have
helped in reducing the computational complexity for calcuating the
PCA.

\medskip

Creating an index for a distributed dataset is very challenging in
mapreduce framework. Since the mapreduce framework relies heavily on
sequential access of data it is very difficult to adapt this
algorithm to the framework.

\medskip

Likewise, There is no easy way to use spark readily for creating
indexes for the dataset. Hence this algorithm cannot be readily
adapted to spark. There are few improvements in spark that is
currently underway to create a indexed RDD but it is not yet
immediately available.

\bigskip

\subsection{Analysis: Gorder}
A great deal of previous research on KNN Join focussed on creating
index for the S dataset, but this paper \cite{xia_gorder:_2004} introduces a
way to optimize the block nested loop join using a method called
GORDER, in which they introduce a new ordering called as grid order.

\medskip

Grid order partitions the dataspace into $l^d$ rectangular cells where
l is number of segments per dimension of the grid. Two points $p_m <
p_n $ if and only if $v_m < v_n$ where $v_m, v_n$ are cells
surrounding $p_m\ and\ p_n$. $v_m < v_n$ if and only if $v_m(s_k) <
v_n(s_k)$ for dimension k and $\forall\ j < k$
\bigskip

In GORDER algorithm, there are two phases
\begin{enumerate}
\item Compute the PCA for both dataset R and S
\item Sort the dataset based on grid order. The resulting dataset is
  grouped into $l^d$ cells, where l is number of segment per
  dimension.
\item Now for each block in R, probable block in S is selected and KNN
  is calculated within the points in those blocks.
\end{enumerate}

\bigskip

Their approach to solve the high dimensionality problem is
by applying Principal Component Analysis to reduce the dimensions. As
mentioned above the major drawback is calcuating PCA for
large datasets is itself a resource and time intensive. There
is no study of effect of approximate PCA on the overall KNN Join results.

\medskip

This algorithm also relies heavily on global sorting of the dataset
which in itself can be quite network bandwidth intensive.

\medskip

This paper also suffers from the same problem as
\cite{yu_efficient_2007} with regards to its probable adoption to
mapreduce framework and Spark.

\bigskip
\subsection{Analysis: zKNN}
In the most recent paper \cite{zhang_efficient_2012}, the authors work on using
MapReduce framework to do parallel KNN Join. In this paper they
describe three algorithms. One very basic algorithm they describe is
called Block Nested Loop Join (BNLJ). In this they partition the R and S
dataset into n partitions and they create $n^2$ buckets
with all combination of R and S partitions. Though this algorithm is
distributed and can run on Hadoop mapreduce framework, this algorithm
performs poorly in all fronts, CPU, Network IO and Disk IO. This is
because the we need have every combination of S and R as possible
which means data replication and transfer of data across nodes. this
is slowest algorithm possible, with complexity of algorithm to be
$O(|R| \times |S|)$

\bigskip

The next algorithm they talk about is Block Nested Loop Join with R
tree index (BRJ). This improves over the Block Nested Loop Join by using R
tree for each S Partition. R tree gives a very good performance for
lower dimensions and it has been used highly effective for lower
dimension \cite{kuan_fast_1997} but for higher dimensions the
performance is almost as bad as a linear scan. R tree is computed once
and stored and it is reloaded everytime it is needed per partition.

\medskip

The complexity of the BRJ algorithm is $O(n|S|log(|S|/n) + n|R|\sqrt{|S|/n})$
Though this is better than the BLNJ,
the shuffle bytes and need to bulk load the R tree index and number of
servers needed is high to be used in any practical implementations.

\bigskip

The same paper they also present an approximate algorithm called zKNN
in Hadoop Mapreduce Framework. zKNN is based on mapping
multi-dimensional data into one dimension using space filling
curves. It was initially described for relational databases in \cite{yao_k_2010}
The fig is taken from \cite{song_solutions_2015}

\begin{figure}[here]
\includegraphics[width=0.5\textwidth]{zknn.png}
\caption{zvalue based partitioning}
\label{fig:zknn.png}
\end{figure}

\bigskip

In zKNN algorithm has the following steps
\begin{enumerate}
\item Transform the data in R and S into single dimension using z order
  curves.
\item Sort and Partition the data in R and S such that the Ri and Si
  has same partition and is sent to same reducers.
\end{enumerate}

\bigskip

The main advantage of the algorithm is its simplicity and is much
faster than other algorithms in map reduce framework
\cite{lu_efficient_2012},
\cite{stupar_rankreduceprocessing_2010}. Also unlike BRJ algorithm it
does not require $n^2$ reducers but only n reducers.

\medskip

Some of the drawbacks of this algorithm evaluated in
\cite{song_solutions_2015} are
\begin{enumerate}
\item Provides only approximate results
\item Accuracy decreases with increasing dimensions. With 30 dimensions
  accuracy is only 60 percent
\item Accuracy decreases with increased data size
\item The amount of shuffle bytes is approximately 10 times the
  original data and has non linear increase with increasing dimensions, accuracy and
  number of neighbours
\item The study has not covered the impact of very high dimensions
  (100s of dimensions) on the accuracy and how to improve it.
\item It is difficult to add an incremental updates to this algorithm
  as it will require to alter the partitions.
\end{enumerate}

\bigskip

Since this algorithm is designed with mapreduce in mind. It should be
easier to adapt the algorithm to spark framework. But there are few
gaps that can be worked on to make this algorithm still faster,
efficient and scalable.

\subsection{Analysis: Rankreduce}

During the same time another study
\cite{stupar_rankreduceprocessing_2010} also focused on getting
approximate KNN Join results in map reduce framework.

\medskip

This algorithm implements the Distributed Locality Sensitive Hashing (LSH)
based index within Mapreduce framework. LSH uses Locality sensitive
Hash functions which transforms any vectors or objects in d
dimensional space with high probability to a hash bucket.

\bigskip

High level steps for this algorithm is
\begin{enumerate}
\item For every point in S dataset, apply Locality sensitive hash
  functions and add it to a hash bucket
\item For every point in R dataset, apply the same hash functions and
  find the hash bucket
\item Find the nearest Neighbour by computing the distance to every
  element in the bucket
\end{enumerate}


\begin{figure}[here]
\includegraphics[width=0.5\textwidth]{rankreduce.png}
\caption{RankReduce Framework}
\label{fig:rankreduce.png}
\end{figure}

\bigskip

The advantage of this algorithm is
\begin{enumerate}
\item Simplicity
\item Better accruacy with increasing dataset
\end{enumerate}

\bigskip

The paper in itself does not talk about the following
\begin{enumerate}
\item Effect of high dimension on its accuracy
\item Effect of high dimension on its shuffle bytes
\item Effect of high dimension on its cpu time
\item Effect of number of neighbours on cputime, accuracy
\item Number of hash buckets in relation to dataset size
\item Number of hash buckets to its accuracy
\item Leverage more nodes for parallel processing
\end{enumerate}

Though the algorithm looks promising in the space of approximate KNN
Join algorithm, still more work needs to be done in order to properly
compare with \cite{zhang_efficient_2012}. On the other hand the
evaluation by \cite{song_solutions_2015} shows that though
\cite{zhang_efficient_2012} has better performance than
\cite{stupar_rankreduceprocessing_2010}, it has lower accuracy.

This algorithm can be adapted for spark framework with ease. But with
all the unanswered questions above there is lots of scope of work in
both mapreduce and spark.

\bigskip

\subsection{Analysis: Voronoi Partition}
Though mapreduce framework has been around for nearly 10 years, there
is only one KNN Join algorithm \cite{lu_efficient_2012} which is
optimized for the mapreduce framework and that provides accurate
results.

\medskip

This algorithm is based on voronoi partition. The algorithm is
divided into 4 Phases
\begin{enumerate}
\item Preprocessing phase: In this phase pivots are selected
\item Data Partitioning Phase: In this phase both R and S dataset is
  partitioned based on the nearest pivot.
\item Join Map Phase: In this phase data points in S is replicated to
  all partitions of R
\item Join Reduce Phase: Nearest neighbours are found based on data
  for each partition
\end{enumerate}

\bigskip

In the preprocessing phase we select the pivots. The paper
investigates on three different ways to select the pivots.
\begin{enumerate}
\item Random Selection
\item Farthest Selection
\item K Means Selection
\end{enumerate}

\medskip

Farthest Selection is worthless in every aspect of performance and
selectivity. Though K Means selection results in better selectivity of partition
and therefore faster computation in Phase 3 and 4, preprocessing time
is slower. Random selection is relatively results in faster
preprocessing time than K Means, but both of them take a significant
amount of time. The computation cost of Random Selection is $O(n^2)$
where n is the number of pivots. This preprocessing phase does not use the
mapreduce framework at all. It would be more efficient if we use map
reduce for this phase as well. One possible way is given in
\cite{kiefer_pairwise_2010}.

\bigskip

Data partitioning phase has two objectives.
\begin{enumerate}
\item Divide the data into parititions in both R and S dataset
\item Generate Summary Table which contains number of elements in each
  partitions, minimum and maximum distance of points from the pivot, K
  Nearest Neighbours of pivots and their distance
\end{enumerate}

\medskip

The computation cost of this phase is $O(|S|*n + |R|*n)$ where n is
the number of pivots. Higher the number of pivots results in longer
data partitioning phase but results in smaller partition. Smaller
parition will be easier to compute the KNN for a partition.

\medskip

There is a scope for improving ther performance of this phase by
leveraging either iDistance indexing \cite{jagadish_idistance:_2005}
or creating a custom range trees or kd trees.

\bigskip

In Join Map Phase, there are 2 different steps
\begin{enumerate}
\item For each partition in R, bounding Region in S is computed. The
  computational complexity is $O(n^2)$ where n is the number of
  partitions. The Summary table generated in previous data
  partitioning phase is used as input.
\item For every point in S, if it is in bounding Region for a
  partition $R_i$, data is replicated to that partition $R_i$. In this way
  every partition $R_i$ in Join Reduce Phase gets all the probable
  points in S that could be a neighbour for a point in $R_i$
\end{enumerate}

The computation complexity of step 2 is $O(|S| * n)$ where n is the
number of partitions. If the number of paritions is less then
partition pruning will be poor. This phase also generates more shuffle
bytes. It also depends on number of dimensions. If the number of
dimensions is 10 and number of partition is 2000, the data replication
is around 80 times. This is one of the primary reason for poor
performance of this algorithm.

\bigskip

In Join Reduce phase, each reducer gets all points from $R_i$ and all
probable points from S. Now we compute the plain KNN for partition
$R_i$.

\medskip

This algorithm performs better than BNLJ or BRJ.

\bigskip

There are lots of scope for improvement in this algorithm.
\begin{enumerate}
\item Create pivots based on S dataset rather than R. Practically, S
  dataset is found to be much stable than R.
\item Perform a Self join with the same partition. This is to leverage
  the general observation that most Nearest neighbours will be within
  the same partition. Only the points near the edge of the partition
  will have Neighbours present in other partition
\item Replicate the data at the edge of the partition into multiple
  partition. This will help self join give a better performance
\item Create an index at each partition so that finding neighbours
  within the partition is faster
\end{enumerate}

Implementation in spark framework with the above gaps addressed can
result in significant performance improvement.

% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex,
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively.
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.

\bigskip

\section{Conclusion}

There are lots of papers focussing both on Index based approach or
Partition based approach. Though Index based approach looks promising
in lower dimension, it fails to perform well
at higher dimension. Partition based approach has been used
predominantly in distributed algorithms. This is mainly because this
easily provides parallelism which is main factor for distributed
computing. There is no work which assessed usage of both index and
partition based approarch in tandem for distributed system.

\bigskip

Though mapreduce has dominated the last decade there is only few algorithms which are implemented in mapreduce
framework. But they are far from efficient in terms of network
bandwidth and disk IO for a large dataset. But as far as spark
framework is concerned there are no work so far.

\bigskip

% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section
\newpage
% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliography{knnjoin}
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)

%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
% \begin{thebibliography}{1}

% \bibitem{IEEEhowto:kopka}
% H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%   0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

% \end{thebibliography}




% that's all folks
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
