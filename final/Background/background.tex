\chapter{Background} \label{chap:Background}

\section{KNN Join}
In this section we will introduce various terminologies and formally
describe what is KNN Join. Historically while describing KNN Join, training dataset is termed as
S dataset and test dataset as R dataset. Henceforth, we will describe
training and test dataset as S and R dataset
respectively. Table ~\ref{table_notations} lists all the symbols that will be
used throughout the paper.

\begin{table}[!ht]
%\renewcommand{\arraystretch}{1.3}
\caption{Notations}
\label{table_notations}
\centering
\begin{tabular}{|c|l|}
\hline
Notation & Meaning \\
\hline
S & Training Dataset \\

R & Test Dataset\\

k & Number of neighbour points \\

d & Number of dimensions \\

n & Number of elements in Dataset \\

c & some positive constant \\

$\mathbb{R}^d$ & Real vector of d dimensions \\

$R_i, S_i$ & Partitions in R and S Respectively \\

r,s & feature Vectors in Dataset R and S Respectively \\

d(r,s) & Distance between two points r and s \\

$O_i$ & Pivot points \\

\hline
\end{tabular}
\end{table}


\textbf{\emph{Definition 1}} \\
Given two datasets R and S, where $R,S \in\
\mathbb{R}^d$ and $\mathbb{R}^d$ is a Real vector of d dimensions, KNN Join returns the K
Nearest Neighbours in S for every feature Vector in R.

\medskip

\begin{center}
     $KNN(R,S) = {(r,s) | \forall r\ \in\ R, \forall s\ \in KNN(r,S)
     } $
\end{center}

Nearest neighbour is calculated by computing the distance between every vector
in R with every vector is S. The distance metric could be any metric
like euclidean distance or manhattan distance.

\medskip

\textbf{\emph{Definition 2}} \\
 Given two datasets R and S where $R,S\ \in\ \mathbb{R}^d$, Approximate KNN Join returns the K
approximate neighbours in S for every feature Vector in R.

\medskip

If $p$ is the exact K\textsuperscript{th} neighbour of r and $p'$ be approximate
K\textsuperscript{th} neighbour of r, then for any $\epsilon$

\bigskip

\begin{center}
$d(p,r) <= d(p',r) <= (1+\epsilon) d(p,r)$
\end{center}

\bigskip

KNN Join is simple to understand and implement but the major draw back
for the algorithm is its computational complexity. The computational
complexity of KNN Join is $ O(|R|*|S|)$. If the size of R and S is high,
say for example 1 Million, the number of computation that needs to be
done is $10^{12}$, which is 1 Trillion calculations. In the age of big data 1
Million dataset is very nominal. Even with this small dataset size, the
running time is too high (approximately 40 hours, if run on 8 core machine), which
deters everyone from using KNN Join. Until late 1990s, KNN Join was
never considered as a feasible algorithm at all, owing to its computational
complexity.There are significant amount of research that has been done to reduce the
computational complexity. Most algorithms that are out today have focussed
on either creating an Indexing Method or Paritioning Method in a standalone system. This is because most of those research predates the
current big data era. In this modern day of high velocity and high volume of data,
only a distributed algorithm with reduced computational complexity will yield the results in resonable amount of time.

The research works that has been done to optimize the KNN algorithm can be categorized based on
\begin{enumerate}
\item Approach
\item Accuracy
\item Scalability
\end{enumerate}



\bigskip

\section{Index Based vs Partition Based KNN}
The current available research for reducing the computational
complexity can be broadly classified into these approaches.
\begin{enumerate}
\item Index Based
\item Partition Based
\end{enumerate}

\medskip

In Index based KNN Join, an index is created for one or both of R and S
dataset. Index is used as a primary way of reducing complexity in \cite{jagadish_idistance:_2005}
\cite{yu_efficient_2007} \cite{bohm_epsilon_2001}.  With indexes it
becomes easier to look for neighbouring elements in dataset S, based
on certain features like distance, boundary cells,  as we
need to compare only a subset of elements which satisfies the
criteria. For example, in distance based index
\cite{jagadish_idistance:_2005} we only compare elements which are at
similar distance in S for an element in R. This can result in huge
reduction in the number of comparison and it can be as low as $log(n)$, where n is the number of element in
dataset S.

\medskip

Indexes does not scale for high dimension data. This is true for various
index algorithms like R tree, $R^* tree$, Quad Tree, Mux tree, B+
tree. Using index for reducing distance compuation with every element, works
remarkably well for 2D or 3D geospatial data. The major drawback with the
index based approach is that when the number of dimensions increase,
creating an index is much costly and its performance is worse than
that of sequential scan. This is because at higher dimensions the distance
between closest and farthest neighbors will be nearly same
\cite{beyer_when_1999}. Another key problem with this approach, even
for lower dimensions, when
converting to distributed environment is that it is difficult to
create an index and use it.

\bigskip

In Partition based KNN Join, the dataset is partitioned into different
non overlapping partitions. The
partition can be distance based\cite{lu_efficient_2012} or hashing
based \cite{stupar_rankreduceprocessing_2010} \cite{yao_k_2010}. For each partition in R only few partitions in S which are
near to R is considered for presence of nearest neighbours. This
method quickly simplifies the number of elements which we need to
compare when we are looking for neighbours.

\medskip

There are few challenges with this approach as well. One
of the main challenge is to find a way to partition all nearby neighbours for a subset
$R_i$ in relatively few subsets of $S_i$. Hash based partition cannot
get partition correctly, which means hash based partition will not
provide accurate KNN Join results. In distance based partition it is difficult to partition
with relatively low standard deviation in size of the partition. If
not partitioned properly it can result in one big partition which
would result in same performance as that of bruteforce.

\bigskip


\section{Centralized vs Distributed KNN}

Based on the scalability the reseach works can be divided into two
groups
\begin{enumerate}
\item Centralized KNN
\item Distributed KNN
\end{enumerate}

Since most work in KNN Join has been done before the big data era,
most of the algorithms are designed to be centralizedd
\cite{jagadish_idistance:_2005} \cite{xia_gorder:_2004}
\cite{bohm_k-nearest_2004} \cite{kuan_fast_1997} \cite{yu_high-dimensional_2010}.
Several recent studies \cite{stupar_rankreduceprocessing_2010},
\cite{lu_efficient_2012} have focused on distributed
processing, which enables those algorithms to be used in bigger
dataset which is not possible otherwise.

\medskip

All the centralized algorithms are designed with the idea that the
data will fit into a single node. With the disk being so cheap, we can
store most of the data within a single node. With falling cost of on
demand availability of compute cluster the algorithms does not take
advantage of parallel processing. Most algorithms which are
centralized builds an index for the entire data. Making those
algorithm distributed is rather difficult.

\medskip

Hadoop Mapreduce was dominant distributed framework for last 8
years. There has been one research work \cite{lu_efficient_2012} which
used mapreduce as its framework for its algorithm. The algorithms
which are crafted for these distributed framework are not easily
portable to other frameworks. But as the
systems evolved, mapreduce framework is on the way out. This is
because mapreduce was designed for scalability. It can scale to handle
petabytes of data with ease. But it was not designed for
performance. Any algorithm if designed for mapreduce framework is
created as a pipeline of map and reduce steps and all the
intermediate results are writtent to disk and read from the disk. This
lowered the performance a lot.

Wit the advent of spark with its inmemory primitives, it is quickly becoming the defacto framework for
distributed computing. There is significant increase in number users
in spark and it has proved its exceptional performance over
hadoop mapreduce. But there are no work that has been done to make
KNN Join better for spark framework. This is an area where significant
work is needed to convert algorithms in centralized settings and mapreduce framework  to spark
framework.

\section{Accurate vs Approximate KNN}

Due to the high computational complexity of KNN Join, researchers
also focused on getting approximate results. Based on the accuracy the
research works can be classified into two groups
\begin{enumerate}
\item Accurate KNN
\item Approximate KNN
\end{enumerate}


Between the accurate and approximate KNN algorithm, there are trade
off between accuracy and the computation cost. There are few
works dedicated to get approximate results \cite{stupar_rankreduceprocessing_2010}
\cite{zhang_efficient_2012} and few for accurate
\cite{jagadish_idistance:_2005} \cite{xia_gorder:_2004}
\cite{lu_efficient_2012}. Some algorithms also allow to control the
level of accuracy $\epsilon$  parameter of the results. With better approximation implies
more computation cost. Choosing either one of the algorithm depends on
application in question.

\medskip

Two methods are predominantly
used in approximate algorithms.
\begin{enumerate}
\item z order curve
\item Locality sensitive Hashing
\end{enumerate}

\medskip

One of the major problem with approximate algorithms is that at higher
dimensions the accuracy becomes very low. The $\epsilon$ paramter to
control the level of accuracy, if it is very low can result in very
poor performance than the accurate one. Only the applications that is
using the algorithm can decide if such trade off is acceptable.

\medskip

\section{Spark}
Apache spark is an opensource distributed framework for cluster
computing designed and developed by University of California,
Berkeley. Hadoop Mapreduce uses two stage disk based Mapreduce
paradigm with two stages being map and reduce. In contrast to that
spark's in memory multi stage primitives provide about 100 times
faster performance for certain applications, if the data can fit into
memory completely. Due to spark's multistage design it is highly
suitable for machine learning and for other iterative algorithms.
Spark supports standalone native cluster manager, Hadoop YARN or
Apache Mesos. We have used Apache Mesos as our cluster manager in our
experiments. Spark can use any distributed storage systems underneath
like HDFS, Cassandra, Amazon S3, etc.

Spark has advanced DAG execution engine which allows developer to
create complex execution pipelines. Spark holds all intermediate
results in memory rather than writing to disk, this helps a lot if you
need to reuse some of the results again and again. It is also designed
to use disk in case if it cannot fit all the data in memory.

Resilient Distributed Dataset(RDD) is the core concept in spark. RDD
is a distributed table which can hold any data. RDD may contain many
partitions and each partition is stored in different nodes if needed.
RDDs are immutable. Any transformations on the RDD returns a new
RDD. So any RDD has a lineage and in case a node contains an RDD goes
down, we can recreate the RDD following the lineage structure. This is
how spark achieves fault tolerance.

RDD Supports two types of operations
\begin{enumerate}
\item Transformation
\item Action
\end{enumerate}

Transformation, as the name denotes, transforms a RDD to a new
one. Some of the transformations are map, flatMap, mapPartitions,
union, intersection, distinct, groupByKey, join, etc. When a
transformation is called it is not evaluated immediately, it is
evaluated only when an action is called.

Action operation on a RDD evaluates and returns a new value. It
evaluates all the Trasformations that are done before the
action. Examples for actions are reduce, collect, take, foreach, etc.

Spark's execution hierarchy has the following elements
\begin{enumerate}
\item Jobs
\item Stages
\item Tasks
\end{enumerate}

Jobs are the top of the hierarchy. A job is triggered when an action
operation is triggered on a RDD. A job has set of stages, which are
set of transformations which can be done without shuffling. If there
is a shuffling then it becomes a next stage. Stage contains set of
tasks which is executing same code on different nodes.

In this thesis we have worked on optimizing the number of jobs and
stages so that there are not many running tasks which can increase the
running time. We also used caching effectively so that the
intermediate results are reused.


\bigskip
